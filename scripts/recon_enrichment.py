"""
–†–µ–∫–æ–Ω–Ω–µ—Å–µ–Ω—Å-–æ–±–æ–≥–∞—â–µ–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–π ‚Äî –º–∞—Å—Å–æ–≤—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Å–∞–π—Ç–æ–≤.

–ë–µ—Ä—ë—Ç –∫–æ–º–ø–∞–Ω–∏–∏ –∏–∑ –ë–î, –∫–æ—Ç–æ—Ä—ã–µ –∏–º–µ—é—Ç —Å–∞–π—Ç (website != NULL),
–ø–∞—Ä—Å–∏—Ç –∏—Ö —á–µ—Ä–µ–∑ web_crawler –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –∫–æ–Ω—Ç–∞–∫—Ç—ã
(emails, —Ç–µ–ª–µ—Ñ–æ–Ω—ã, —Å–æ—Ü—Å–µ—Ç–∏, –ò–ù–ù) –æ–±—Ä–∞—Ç–Ω–æ –≤ —Ç–∞–±–ª–∏—Ü—É contacts.

–ó–∞–ø—É—Å–∫:
    python scripts/recon_enrichment.py [--limit 50] [--force]
"""
import sys
import os
import time
import argparse

# Encoding fix for Windows
sys.stdout.reconfigure(encoding='utf-8', errors='replace')
sys.stderr.reconfigure(encoding='utf-8', errors='replace')

# Add project root to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import psycopg2
import psycopg2.extras
from dotenv import load_dotenv

load_dotenv()

# ‚îÄ‚îÄ‚îÄ Database connection ‚îÄ‚îÄ‚îÄ
# NOTE: .env –∏–º–µ–µ—Ç POSTGRES_DB=b2b_intelligence, –Ω–æ —Ä–µ–∞–ª—å–Ω–∞—è –ë–î –≤ Docker = marketai
DB_CONN = {
    "host": os.getenv("DB_HOST", "localhost"),
    "port": int(os.getenv("DB_PORT", 5432)),
    "database": "b2b_intelligence",
    "user": os.getenv("POSTGRES_USER", "marketai"),
    "password": os.getenv("POSTGRES_PASSWORD", "marketai"),
}

# Import our existing web crawler
from src.recon.web_crawler import crawl_website


def get_companies_to_crawl(conn, limit=50, force=False):
    """–ü–æ–ª—É—á–∏—Ç—å –∫–æ–º–ø–∞–Ω–∏–∏ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞."""
    cur = conn.cursor(cursor_factory=psycopg2.extras.DictCursor)
    
    if force:
        # –ü–∞—Ä—Å–∏—Ç—å –≤—Å–µ —Å —Å–∞–π—Ç–æ–º
        cur.execute("""
            SELECT id, name, website, inn 
            FROM companies 
            WHERE website IS NOT NULL AND website != ''
            ORDER BY revenue_total DESC NULLS LAST
            LIMIT %s
        """, (limit,))
    else:
        # –ü–∞—Ä—Å–∏—Ç—å —Ç–æ–ª—å–∫–æ —Ç–µ, —É –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤ —Å –∏—Å—Ç–æ—á–Ω–∏–∫–æ–º web_crawl
        cur.execute("""
            SELECT c.id, c.name, c.website, c.inn 
            FROM companies c
            WHERE c.website IS NOT NULL AND c.website != ''
              AND c.id NOT IN (
                  SELECT DISTINCT company_id FROM contacts WHERE source = 'web_crawl'
              )
            ORDER BY c.revenue_total DESC NULLS LAST
            LIMIT %s
        """, (limit,))
    
    rows = cur.fetchall()
    cur.close()
    return rows


def save_crawl_results(conn, company_id, crawl_result):
    """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤ –ë–î."""
    cur = conn.cursor()
    saved = 0
    
    # –£–¥–∞–ª–∏—Ç—å —Å—Ç–∞—Ä—ã–µ web_crawl –∫–æ–Ω—Ç–∞–∫—Ç—ã –¥–ª—è —ç—Ç–æ–π –∫–æ–º–ø–∞–Ω–∏–∏
    cur.execute("DELETE FROM contacts WHERE company_id = %s AND source = 'web_crawl'", (company_id,))
    
    # Emails
    for email in crawl_result.emails:
        cur.execute("""
            INSERT INTO contacts (company_id, type, value, source, is_verified)
            VALUES (%s, 'email', %s, 'web_crawl', false)
        """, (company_id, email))
        saved += 1
    
    # Phones
    for phone in crawl_result.phones:
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        clean = phone.strip()
        cur.execute("""
            INSERT INTO contacts (company_id, type, value, source, is_verified)
            VALUES (%s, 'phone', %s, 'web_crawl', false)
        """, (company_id, clean))
        saved += 1
    
    # Social links
    for platform, url in crawl_result.social_links.items():
        cur.execute("""
            INSERT INTO contacts (company_id, type, value, label, source, is_verified)
            VALUES (%s, %s, %s, %s, 'web_crawl', false)
        """, (company_id, platform, url, platform.capitalize()))
        saved += 1
    
    # –û–±–Ω–æ–≤–∏—Ç—å –ò–ù–ù –µ—Å–ª–∏ –Ω–∞—à–ª–∏ –∏ –Ω–µ—Ç –≤ –ë–î
    if crawl_result.inn:
        cur.execute("""
            UPDATE companies SET inn = %s WHERE id = %s AND (inn IS NULL OR inn = '')
        """, (crawl_result.inn, company_id))
    
    conn.commit()
    cur.close()
    return saved


def run_mass_crawl(limit=50, force=False):
    """–ó–∞–ø—É—Å—Ç–∏—Ç—å –º–∞—Å—Å–æ–≤—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Å–∞–π—Ç–æ–≤ –∫–æ–º–ø–∞–Ω–∏–π."""
    print("=" * 60)
    print("üîç –†–ï–ö–û–ù–ù–ï–°–ï–ù–° ‚Äî –ü–∞—Ä—Å–∏–Ω–≥ —Å–∞–π—Ç–æ–≤ –∫–æ–º–ø–∞–Ω–∏–π")
    print("=" * 60)
    
    conn = psycopg2.connect(**DB_CONN)
    companies = get_companies_to_crawl(conn, limit=limit, force=force)
    
    print(f"\nüìã –ö–æ–º–ø–∞–Ω–∏–π –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞: {len(companies)}")
    if not companies:
        print("‚úÖ –í—Å–µ –∫–æ–º–ø–∞–Ω–∏–∏ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã!")
        conn.close()
        return {"processed": 0, "contacts_found": 0}
    
    total_contacts = 0
    processed = 0
    errors = 0
    
    for i, company in enumerate(companies, 1):
        comp_id = company['id']
        name = company['name']
        website = company['website']
        
        print(f"\n[{i}/{len(companies)}] üè¢ {name}")
        print(f"    üåê {website}")
        
        try:
            result = crawl_website(website, max_depth=2, max_pages=10)
            
            # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            saved = save_crawl_results(conn, comp_id, result)
            total_contacts += saved
            processed += 1
            
            print(f"    üìß Emails: {len(result.emails)}")
            print(f"    üìû –¢–µ–ª–µ—Ñ–æ–Ω—ã: {len(result.phones)}")
            print(f"    üîó –°–æ—Ü—Å–µ—Ç–∏: {list(result.social_links.keys())}")
            if result.inn:
                print(f"    üèõÔ∏è –ò–ù–ù: {result.inn}")
            print(f"    üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤: {saved}")
            
        except Exception as e:
            errors += 1
            print(f"    ‚ùå –û—à–∏–±–∫–∞: {e}")
        
        # –ü–∞—É–∑–∞ –º–µ–∂–¥—É —Å–∞–π—Ç–∞–º–∏
        time.sleep(1)
    
    conn.close()
    
    print("\n" + "=" * 60)
    print(f"‚úÖ –†–ï–ó–£–õ–¨–¢–ê–¢:")
    print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed}/{len(companies)}")
    print(f"   –ö–æ–Ω—Ç–∞–∫—Ç–æ–≤ –Ω–∞–π–¥–µ–Ω–æ: {total_contacts}")
    print(f"   –û—à–∏–±–æ–∫: {errors}")
    print("=" * 60)
    
    return {
        "processed": processed,
        "contacts_found": total_contacts,
        "errors": errors
    }


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="–ú–∞—Å—Å–æ–≤—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Å–∞–π—Ç–æ–≤ –∫–æ–º–ø–∞–Ω–∏–π")
    parser.add_argument("--limit", type=int, default=50, help="–ú–∞–∫—Å–∏–º—É–º –∫–æ–º–ø–∞–Ω–∏–π –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞")
    parser.add_argument("--force", action="store_true", help="–ü–µ—Ä–µ–ø–∞—Ä—Å–∏—Ç—å –¥–∞–∂–µ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ")
    args = parser.parse_args()
    
    run_mass_crawl(limit=args.limit, force=args.force)
